\section{Data}

Our study examines the adoption of words on social media, and we focus on Reddit as a source of language change. 
Reddit is a social content sharing site separated into distinct sub-communities or ``subreddits'' that center around particular topics~\cite{gilbert2013}. 
Reddit is a socially diverse and dynamic online platform, making it an ideal environment for research on language change~\cite{kershaw2016}.
Furthermore, because Reddit data is publicly available we expect that this study can be more readily replicated than a similar study on other platforms such as Facebook or Twitter, whose data is less easily obtained.

We analyze a set of public monthly Reddit comments\footnote{From \url{http://files.pushshift.io/reddit/comments/} (Accessed 1 October 2016).} posted between 1 June 2013 and 31 May 2016, totalling $T=36$ months of data.
This dataset has been analyzed in prior work~\cite{hessel2016,tan2015} and has been noted to have some missing data~\cite{gaffney2018}, although this issue should not affect our analysis.
To reduce noise in the data, we filter all comments generated by known bots and spam users\footnote{The same list used in~\newcite{tan2015}: \url{https://chenhaot.com/data/multi-community/README.txt} (Accessed 1 October 2016).} 
and filter all comments created in well-known non-English subreddits.\footnote{We randomly sampled 100 posts from the top 500 subreddits and labelled a subreddit as non-English if fewer than 90\% of its posts were identified by \texttt{langid.py}~\cite{lui2012} as English.}
%We also filter all comments that had been deleted by the time of collection (1 October 2016). 
%These filtering steps reduce the data by 4.6\% of its original size. 
The final data collected is summarized in Table \ref{tab:data_summary}.

\begin{table}
\small
\centering
\input{data-summary-table}
    \caption{Data summary statistics.}
    \label{tab:data_summary}
\end{table}

%The increasing popularity of Reddit over the time period of collected data is clear in Figure \ref{fig:commentUserTime}, which shows the number of comments and users over time. 
%To account for this growing popularity, we use normalized frequency in this study.
%
%\begin{figure}[t!]
%\includegraphics[width=0.9\columnwidth]{figures/comment_users_time.png}
%\caption{Comments and users on Reddit across time.}
%\label{fig:commentUserTime}
%\end{figure}

%To reduce data sparsity,
We replace all references to subreddits and users (marked by the convention \example{r/subreddit} and \example{u/user}) with \example{r/SUB} and \example{u/USER} tokens, and all hyperlinks with a \example{URL} token. 
We also reduce all repeated character sequences to a maximum length of three (e.g., \example{loooool} to \example{loool}).
The final vocabulary includes the top 100,000 words by frequency.\footnote{We restricted the vocabulary because of the qualitative analysis required to identify nonstandard words.}
%This reduces the risk of data sparsity but may exclude rare innovations.} 
We replace all OOV words with UNK tokens, which comprise 3.95\% of the total tokens.
% scripts/data_processing/count_oov_tokens.py

\subsection{Finding growth words}
\label{subsec:growth_words}

%\begin{figure*}[t!]
%\centering
%\includegraphics[width=\textwidth]{figures/growth_decline_piecewise_logistic_example.pdf}
%\caption{Detecting decline words by fitting a linear piecewise function (left) and a logistic function (right). }
%\label{fig:failure-examples}
%\end{figure*}

Our work seeks to study the growth of nonstandard words, which we identify manually instead of relying on pre-determined lists~\cite{tredici2018}.% such as the acronym \example{tbh} (\gloss{to be honest}).
%, which are not uncommon in a dynamic environment such as Reddit \jacob{cite for this claim?}. 
%We mark a word $w$ as an growing innovation if it fulfills one of two criteria with respect to the time period with $N$ months:
%
%\begin{enumerate}
%\item The ratio of the minimum final frequency (in the first $k=3$ months) to the maximum starting frequency (in the final $k$ months) exceeds a threshold $f_{\theta}$: $\frac{min(f_{w,0:k})}{max(f_{w,N-k:N})} > f_{\theta}$, or
%\item the Spearman correlation coefficient for monthly frequency across the entire time period exceeds a lower threshold correlation $\rho_{\theta}$: $\rho_{w,N} > \rho_{\theta}$.
%\end{enumerate}
To detect such words, we first compute the Spearman correlation coefficient 
%$\rho$ 
between the time steps $\{1...T\}$ and each word $w$'s frequency time series $f^{(w)}_{(1:T)}$ (frequency normalized and log-transformed).
The Spearman correlation coefficient captures monotonic, gradual growth that characterizes the adoption of nonstandard words~\cite{grieve2016,kershaw2016}.
%a time series that increases constantly has a Spearman correlation coefficient of 1.0, while a time series that decreases constantly has a Spearman correlation coefficient of -1.0.
%\ian{the thresholds were based on 95\% percentiles...how do we justify choosing that cutoff?}

%These two growth metrics capture monotonic, gradual growth as well as sudden growth that characterize the adoption of lexical innovations~\cite{grieve2016,kenter2015}. 
%We first filter for words whose Spearman correlation coefficient exceeded the $95^{th}$ percentile ($N=4941$).
The first set of words is filtered by a Spearman correlation coefficient above the $85^{\text{th}}$ percentile ($N=15,017$).
%%ok -j
%\ian{we actually didn't use a dictionary to filter because that gets rid of edge cases like \example{doe} (from \gloss{though})}
%then remove all words that appear in a standard English dictionary\footnote{Found here: From the Summer Institute for Linguistics: \url{http://www-01.sil.org/linguistics/wordlists/english/wordlist/wordsEn.txt} (Accessed 17 June 2017).} to produce a set of nonstandard words.
%We remove all proper nouns and political words such as \example{berniebros},\footnote{Determined through high concentration in political subreddits such as r/politics.} which were correlated with the increased discussion of the 2016 presidential election.
From this set of words, one of the authors manually identified 1,120 words in set $\set{G}$ (``growth'') that are neither proper nouns (\example{berniebot, killary, drumpf}) nor standard words (\example{election, voting}).\footnote{Code and word lists available at: \\ \url{https://github.com/ianbstewart/nonstandard_word_dissemination}.} 
These words were removed because their growth may be due to exogenous influence.
%rather than sociolinguistic factors.
A ``standard'' word is one that can plausibly be found in a newspaper article, which follows from the common understanding of newspaper text as a more formal and standard register.
Therefore, a ``nonstandard'' word is one that cannot plausibly be found in a newspaper article, a judgment often used by linguists to determine what counts as slang~\cite{dumas1978}.
In ambiguous cases, one of the authors inspected a sample of comments that included the word.
We validate this process by having both authors annotate the top 200 growth candidates for standard/proper versus nonstandard (binary), obtaining inter-annotator agreement of $\kappa$=0.79.

\subsection{Finding decline words}
To determine what makes the growth words successful, we need a control group of ``decline'' words, which are briefly adopted and later abandoned.
Although these words may have been successful before the time period investigated, their decline phase makes them a useful comparison for the growth words. 
We find such words by fitting two parametric models to the frequency series.

\paragraph{Piecewise linear fit}
We fit a two-phase piecewise linear regression on each word's frequency time series $f_{(1:T)}$, which splits the time series into $f_{(1:\hat{t})}$ and $f_{(\hat{t}+1:T)}$.
%where split point $\hat{t}$ is a free parameter. 
The goal is to select a split point $\hat{t}$ to minimize the sum of the squared error between observed frequency $f$ and predicted frequency $\hat{f}$:
\begin{equation}
\hat{f}(m_{1}, m_{2}, b,  t) = 
\begin{cases}
b + m_{1}t & t \leq \hat{t} \\
b + m_1 \hat{t} + m_{2}(t - \hat{t}) & t > \hat{t},
\end{cases}
\end{equation}
where $b$ is the intercept, $m_{1}$ is the slope of the first phase, and $m_{2}$ is the slope of the second phase.
Decline words $\set{D}_{p}$ (``piecewise decline'') display growth in the first phase ($m_{1} > 0$), decline in the second phase ($m_{2} < 0$), and a strong fit between observed and predicted data, indicated by $R^{2}(f, \hat{f})$ above the $85^{th}$ percentile (36.1\%); this filtering yields 14,995 candidates.
% scripts/frequency/find_fail_words_from_params.ipynb
%An example decline word \example{wot} (respelling; \gloss{what}) and its piecewise fit are shown in the left panel of \autoref{fig:failure-examples}.
%according to the following criteria, using the slope of the first line $m_{1}$, slope of the second line $m_{2}$, and coefficient of determination $R^{2}$:

\paragraph{Logistic fit}
To account for smoother growth-decline trajectories, we also fit the growth curve to a logistic distribution, %\ian{formula? introducing $\mu$ early}.
which is a continuous unimodal distribution with support over the non-negative reals. 
We identify the set of candidates $\set{D}_{l}$ (``logistic decline'') as words with a strong fit to this distribution, as indicated by $R^{2}$ above the $99^{th}$ percentile (82.4\%), yielding 998 candidates. 
% scripts/frequency/find_fail_words_from_params.ipynb
%An example word \example{iifym} (acronym; \gloss{if it fits your macros}) is shown in the right panel of \autoref{fig:failure-examples}. 
The logistic word set partially overlaps with the piecewise set, because some words' frequency time series show a strong fit to both the piecewise function and the logistic distribution.

%\begin{table*}[t!]
%\centering
%  \input{examples}
%    \caption{Examples of growth words with high and low dissemination values.}
%\label{tab:predictor-example-words}
%\end{table*}

\paragraph{Combined set} 
We combine the sets $\mathcal{D}_{p}$ and $\mathcal{D}_{l}$ to produce a set of decline word candidates ($N=15,665$).
% scripts/frequency/find_fail_words_from_params
Next, we filter this combined set to exclude standard words and proper nouns, yielding a total of 530 decline words in set $\mathcal{D}$.
Each word is assigned a split point $\hat{t}$ based on the estimated time of switch between the growth phase and the decline phase, which is the split point $\hat{t}$ for piecewise decline words and the center of the logistic distribution $\hat{\mu}$ for the logistic decline words.
%This split point is used in our analysis 
%to match successful to failed words in~\autoref{sec:results-classification} 
%of word survival in~\autoref{sec:results-survival}.

\begin{table}
\small
\centering
\begin{tabular}{l p{4.7cm}}
  \toprule
  Word set & Examples \\
  \midrule
$\mathcal{G}$ & \example{idk, lmao, shitpost, tbh, tho} \\
$\mathcal{D}_{l}$ & \example{atty, eyebleach, iifym, obeasts, trashy} \\
$\mathcal{D}_{p}$ & \example{brojob, nparent, rekd, terpers, wot} \\
\bottomrule
\end{tabular}
\caption{Examples of nonstandard words in all word sets: growth ($\mathcal{G}$), logistic decline ($\mathcal{D}_{l}$) and piecewise decline ($\mathcal{D}_{p}$).}
\label{tab:example_growth_decline_words}
\end{table}
% from scripts/frequency/get_top_fitting_growth_decline_words.ipynb

Examples of both growth and decline words are shown in \autoref{tab:example_growth_decline_words}. 
The growth words include several acronyms (\example{tbh}, \gloss{to be honest}; \example{lmao}, \gloss{laughing my ass off}), while the decline words include clippings (\example{atty}, \gloss{atomizer}), respellings (\example{rekd}, \gloss{wrecked}; \example{wot}, \gloss{what}) and compounds (\example{nparent}, \gloss{narcissistic parent}).

\begin{table}
\small
\centering
\begin{tabular}{l p{1.1cm} p{1.1cm} p{1.1cm} p{0.9cm} l}
\toprule
 & Clipping & Compound & Respelling & Other & Total \\
\midrule
$\mathcal{G}$ & 198 (17.7\%) & 334 (29.8\%) & 83 (7.4\%) & 505 (45.1\%) & 1,120 \\ 
% 0.177 & 0.299 & 0.074 & 0.449
% 0.098 & 0.185 & 0.2 & 0.517
  $\mathcal{D}$ & 53 (10.0\%) & 100 (18.9\%) & 108 (20.4\%) & 269 (50.8\%) & 530 \\
  \bottomrule
% scripts/frequency/get_growth_decline_word_category_counts.ipynb
\end{tabular}
\caption{Word formation category counts in growth ($\mathcal{G}$) and decline ($\mathcal{D}$) word sets.}
\label{fig:word_formation_category_counts}
\end{table}

We also provide a distribution of the words across word generation categories in \autoref{fig:word_formation_category_counts}, including compounds and clippings in similar proportions to prior work~\cite{kulkarni2018}.
Because the growth and decline words exhibit similar proportions of category counts, we do not expect that this will be a significant confound in differentiating growth from decline.
%The time series for these words are visualized in the appendix (\autoref{fig:example_time_series}).
%For piecewise failures, $\hat{t}$ is set to the point that optimizes the overall piecewise fit of $m_{1}$ and $m_{2}$. 
%For logistic failures, $\hat{t}$ is set to the center of the logistic distribution.

%\ian{details: I made a score function to account for the slopes and R2, then filtered for the top-scoring words based on how their growth-decline trajectories looked, above the $50^{th}$ percentile}
%This set of candidates is then filtered to only innovations in the same word formation categories as the successful innovations, yielding 557 failure innovations (example shown in Figure \ref{fig:failure_piecewise_example}).

%While this set of failure innovations is considerable, we also want to account for the ``S-curve'' of language change that can be modeled with a logistic function~\cite{kroch1989,tagliamonte2007}.
%We therefore extract another set of innovations that demonstrate a strong fit to a logistic distribution\footnote{$\phi(t; \mu, \sigma) = \frac{\exp(-\frac{t - \mu}{\sigma})}{\sigma(1 - \exp(-\frac{t - \mu}{\sigma}))^{2}}$ where $t$ is the number of months elapsed since June 2013.}.
%following the intuition that innovation adoption and abandonment follows a logistic curve~\cite{kroch1989}. 

%Filtering for strong-fit curves\footnote{Determined by $R^{2}$ score above the $95^{th}$ percentile.} yields an additional 45 innovations for a total of 602 failure innovations in vocabulary $\Gamma$ (example shown in Figure \ref{fig:failure_logistic_example}).
